"""
Evaluate previously generated edits with configurable scorers.

This script reads edits from a JSONL file (generated by generate_edits.py) and
evaluates them with different scorer configurations. This allows you to test
which scorers are most important without regenerating edits.

All scorer thresholds are defined in the scorer classes themselves.

Usage:
    # Evaluate with all scorers enabled
    python models/evaluate_edits.py --input_jsonl <input_file.jsonl> --output_jsonl <output_file.jsonl>

    # Evaluate with specific scorers disabled (ablation study)
    python models/evaluate_edits.py --input_jsonl <input_file.jsonl> --output_jsonl <output_file.jsonl> \
        --disable_human_like --disable_fluency
"""

import os
import sys
import time
import logging
import json
import re
from typing import List, Dict, Any, Optional
import argparse
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import torch
import numpy as np
from sentence_transformers import SentenceTransformer
from bert_score import BERTScorer
from transformers import AutoModelForCausalLM, AutoTokenizer

from scorers.local_scorers.fluency.fluency_scorer import FluencyScorer
from scorers.appropriateness.appropriateness_scorer import AppropriatenessScorer
from scorers.local_scorers.semantic_similarity.semantic_similarity_scorer import SemanticSimilarityScorer
from scorers.local_scorers.human_like.human_like_scorer import HumanLikeScorer
from scorers.global_scorers.semantic_similarity.global_semantic_similarity_scorer import GlobalSemanticSimilarityScorer
from scorers.global_scorers.human_like.global_human_like_scorer import GlobalHumanLikeScorer
from scorers.global_scorers.fluency.global_fluency_scorer import GlobalFluencyScorer
from ops.edit_applier import apply_edits_to_argument

# -----------------------------
# Logging
# -----------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(name)s | %(message)s",
)
logger = logging.getLogger(__name__)

# Analysis dimensions
_ANALYSIS_DIMS = [
    "Inappropriateness",
    "Toxic Emotions",
    "Missing Commitment",
    "Missing Intelligibility",
    "Other Reasons",
]


# -----------------------------
# Perplexity calculation
# -----------------------------
_ppl_tokenizer = AutoTokenizer.from_pretrained("gpt2")
_ppl_model = AutoModelForCausalLM.from_pretrained("gpt2")
_ppl_tokenizer.pad_token = _ppl_tokenizer.eos_token
_PPL_MAX_TOKENS = 1024


def calculate_text_perplexity(text: str) -> Optional[float]:
    """Calculate perplexity of a text using GPT-2."""
    if not isinstance(text, str) or len(text.strip()) == 0:
        return None
    inputs = _ppl_tokenizer(text, return_tensors="pt", truncation=True, max_length=_PPL_MAX_TOKENS).to(_ppl_model.device)
    with torch.no_grad():
        outputs = _ppl_model(**inputs, labels=inputs["input_ids"])
    loss = outputs.loss
    perplexity = torch.exp(loss).item()
    return perplexity


# -----------------------------
# Similarity helpers (NES)
# -----------------------------
def _levenshtein_distance(a: list[str], b: list[str]) -> int:
    if a == b:
        return 0
    if len(a) == 0:
        return len(b)
    if len(b) == 0:
        return len(a)
    dp = [[0] * (len(b) + 1) for _ in range(len(a) + 1)]
    for i in range(len(a) + 1):
        dp[i][0] = i
    for j in range(len(b) + 1):
        dp[0][j] = j
    for i in range(1, len(a) + 1):
        for j in range(1, len(b) + 1):
            cost = 0 if a[i - 1] == b[j - 1] else 1
            dp[i][j] = min(
                dp[i - 1][j] + 1,
                dp[i][j - 1] + 1,
                dp[i - 1][j - 1] + cost,
            )
    return dp[len(a)][len(b)]


def _normalized_edit_similarity_words(before: str, after: str) -> float:
    before_tokens = before.split()
    after_tokens = after.split()
    dist = _levenshtein_distance(before_tokens, after_tokens)
    denom = max(1, max(len(before_tokens), len(after_tokens)))
    return 1.0 - (dist / denom)


# -----------------------------
# Reward computation per edit
# -----------------------------
def score_edit(
    original_argument: str,
    edit: Dict[str, Any],
    baseline_scores: Dict[str, float] | None,
    original_sentence_context: Optional[str],
    semantic_similarity_scorer,
    fluency_scorer,
    human_like_scorer,
    appropriateness_scorer
) -> Dict[str, Any]:
    """Score a single edit with the provided scorers."""
    reason = edit.get("reason")
    inappropriate_part = edit.get("inappropriate_part")
    rewritten_part = edit.get("rewritten_part")

    # Validity check
    edit_context = original_sentence_context if original_sentence_context is not None else original_argument
    has_reason = bool(reason)
    has_at_least_one = bool(inappropriate_part) or bool(rewritten_part)
    substring_match = (not inappropriate_part) or (inappropriate_part in edit_context)
    is_well_formed = has_reason and has_at_least_one and substring_match

    logger.info(f"Scoring edit: inappropriate_part='{inappropriate_part}', rewritten_part='{rewritten_part}', reason='{reason}'")
    logger.info(f"is_well_formed: {is_well_formed} (reason={has_reason}, has_content={has_at_least_one}, substring_match={substring_match})")

    semantic_similarity = 0.0
    fluency_score = 0.0
    human_like = 0.0
    app_reward = 0.0
    reason_correct = False
    classifier_true_reason = None

    if is_well_formed:
        # Semantic similarity
        if semantic_similarity_scorer:
            try:
                semantic_similarity, ss_score = semantic_similarity_scorer.calculate_semantic_similarity(
                    original_argument, inappropriate_part, rewritten_part
                )
                logger.info(f"Semantic similarity: binary={semantic_similarity}, score={ss_score}")
            except Exception as e:
                semantic_similarity = 0.0
                logger.error(f"Semantic similarity check failed: {e}")

        # Fluency
        if fluency_scorer:
            try:
                context = original_sentence_context if original_sentence_context is not None else original_argument
                fluency_score = fluency_scorer.calculate_fluency(context, inappropriate_part, rewritten_part)
                logger.info(f"Fluency Output: {fluency_score}")
            except Exception as e:
                fluency_score = 0.0
                logger.error(f"Fluency check failed: {e}")

        # Human-like
        if human_like_scorer:
            try:
                context = original_sentence_context if original_sentence_context is not None else original_argument
                human_like = human_like_scorer.calculate_human_likeness(
                    original_argument, context, inappropriate_part, rewritten_part
                )
                logger.info(f"Human-like score: {human_like}")
            except Exception as e:
                human_like = 0.0
                logger.error(f"Human-like check failed: {e}")

        # Edit-level appropriateness classifier reward
        if appropriateness_scorer:
            try:
                if original_sentence_context and inappropriate_part in original_sentence_context:
                    modified_sentence = original_sentence_context.replace(inappropriate_part, rewritten_part, 1)
                    modified_argument = original_argument.replace(original_sentence_context, modified_sentence, 1)
                else:
                    modified_argument = original_argument.replace(inappropriate_part, rewritten_part, 1)
                before_scores = baseline_scores if baseline_scores is not None else appropriateness_scorer.get_appropriateness_scores(original_argument)
                after_scores = appropriateness_scorer.get_appropriateness_scores(modified_argument)

                # Compute improvement for overall inappropriateness
                dim_before = before_scores.get("Inappropriateness")
                dim_after = after_scores.get("Inappropriateness")
                if dim_before is not None and dim_after is not None:
                    app_reward = 1.0 if (dim_after < dim_before) else 0.0

                # Evaluate reason correctness
                dimension_improvements = {}
                for dim in _ANALYSIS_DIMS[1:]:
                    dim_before_val = before_scores.get(dim)
                    dim_after_val = after_scores.get(dim)
                    if dim_before_val is not None and dim_after_val is not None:
                        improvement = dim_before_val - dim_after_val
                        dimension_improvements[dim] = improvement

                positive_improvements = {
                    dim: imp for dim, imp in dimension_improvements.items() if imp > 0
                }

                if positive_improvements:
                    classifier_true_reason = max(positive_improvements, key=positive_improvements.get)
                    reason_correct = (reason == classifier_true_reason)
                    logger.info(f"Reason evaluation: predicted='{reason}', true='{classifier_true_reason}', correct={reason_correct}")

            except Exception as e:
                app_reward = 0.0
                reason_correct = False
                classifier_true_reason = None
                logger.error(f"App reward check failed: {e}")

    # Perfect reward: all enabled scorers must pass
    enabled_scores = []
    if semantic_similarity_scorer:
        enabled_scores.append(semantic_similarity)
    if fluency_scorer:
        enabled_scores.append(fluency_score)
    if human_like_scorer:
        enabled_scores.append(human_like)

    perfect = 1.0 if (is_well_formed and all(s == 1.0 for s in enabled_scores) and enabled_scores) else 0.0
    logger.info(f"Perfect score: {perfect}")

    return {
        "reason": reason,
        "classifier_true_reason": classifier_true_reason,
        "inappropriate_part": inappropriate_part,
        "rewritten_part": rewritten_part,
        "valid": bool(is_well_formed),
        "reason_correct": bool(reason_correct),
        "rewards": {
            "semantic_similarity": float(semantic_similarity),
            "fluency": float(fluency_score),
            "human_like": float(human_like),
            "app": float(app_reward),
            "perfect": float(perfect),
        },
    }


def main(
    input_jsonl: str,
    output_jsonl: str,
    disable_semantic_similarity: bool = False,
    disable_fluency: bool = False,
    disable_human_like: bool = False,
    disable_appropriateness: bool = False,
):
    """Evaluate edits from input JSONL and save scored results to output JSONL."""
    os.makedirs(os.path.dirname(output_jsonl), exist_ok=True)

    logger.info(f"Starting evaluation. Input: {input_jsonl}, Output: {output_jsonl}")

    # Initialize scorers based on configuration
    _cuda_available = torch.cuda.is_available()
    _local_rank = int(os.environ.get("LOCAL_RANK", 0)) if _cuda_available else 0
    _device = torch.device(f"cuda:{_local_rank}" if _cuda_available else "cpu")

    logger.info("Loading scorers (using default thresholds from scorer classes)...")
    # All scorers use their default thresholds defined in the class __init__
    semantic_similarity_scorer = None if disable_semantic_similarity else SemanticSimilarityScorer(_device)
    human_like_scorer = None if disable_human_like else HumanLikeScorer(_device)
    fluency_scorer = None if disable_fluency else FluencyScorer(_device)
    appropriateness_scorer = None if disable_appropriateness else AppropriatenessScorer(_device)

    # Global scorers (use their default thresholds)
    logger.info("Loading global scorers...")
    global_semantic_similarity_scorer = GlobalSemanticSimilarityScorer(_device)
    global_human_like_scorer = GlobalHumanLikeScorer(_device)
    global_fluency_scorer = GlobalFluencyScorer(_device)

    # BERTScorer for document-level similarity
    bert_scorer = BERTScorer(model_type="microsoft/deberta-xlarge-mnli", rescale_with_baseline=True, lang="en", batch_size=1, device=_device)

    # Aggregate metrics
    num_examples = 0
    total_edits = 0
    total_valid_edits = 0
    total_perfect_reward_ones = 0
    total_reason_correct = 0
    flips_per_dim: Dict[str, int] = {dim: 0 for dim in _ANALYSIS_DIMS}

    start_all = time.time()
    with open(input_jsonl, "r", encoding="utf-8") as f_in, open(output_jsonl, "w", encoding="utf-8") as f_out:
        for idx, line in enumerate(f_in):
            example_start = time.time()
            try:
                example = json.loads(line.strip())
            except json.JSONDecodeError as e:
                logger.error(f"Failed to parse line {idx}: {e}")
                continue

            num_examples += 1

            # Extract data from saved record
            post_id = example.get("post_id")
            issue = example.get("issue", "")
            argument = example.get("argument", "")
            sentences = example.get("sentences", [])
            completion = example.get("completion", "")
            all_edits = example.get("edits", [])

            # Score each edit
            scored_edits = []
            baseline_cls_scores = appropriateness_scorer.get_appropriateness_scores(argument) if appropriateness_scorer else {}

            for idx_edit, edit in enumerate(all_edits):
                logger.info(f"Scoring edit {idx_edit + 1}/{len(all_edits)}: {edit}")
                sentence_id = edit.get("sentence_id", 0)
                try:
                    original_sentence = sentences[sentence_id - 1] if 0 < sentence_id <= len(sentences) else None
                except (IndexError, TypeError):
                    original_sentence = None

                scored_edit = score_edit(
                    argument,
                    edit,
                    baseline_scores=baseline_cls_scores,
                    original_sentence_context=original_sentence,
                    semantic_similarity_scorer=semantic_similarity_scorer,
                    fluency_scorer=fluency_scorer,
                    human_like_scorer=human_like_scorer,
                    appropriateness_scorer=appropriateness_scorer
                )
                scored_edit['original_sentence'] = original_sentence or ""
                scored_edit['sentence_id'] = sentence_id
                scored_edits.append(scored_edit)

            logger.info(f"Scored {len(scored_edits)} edits")

            # Collect statistics
            valid_count = sum(1 for e in scored_edits if e.get("valid"))
            perfect_ones = sum(1 for e in scored_edits if e.get("rewards", {}).get("perfect") == 1.0)
            reason_correct_count = sum(1 for e in scored_edits if e.get("reason_correct"))

            total_edits += len(scored_edits)
            total_valid_edits += valid_count
            total_perfect_reward_ones += perfect_ones
            total_reason_correct += reason_correct_count

            # Build argument after applying perfect edits
            perfect_edits = [
                {
                    'original_sentence': e.get('original_sentence', ''),
                    'inappropriate_part': e.get('inappropriate_part'),
                    'rewritten_part': e.get('rewritten_part')
                }
                for e in scored_edits
                if e.get("rewards", {}).get("perfect") == 1.0
            ]
            argument_after_edits = apply_edits_to_argument(perfect_edits, sentences, argument)

            # Build argument after applying all valid edits
            all_valid_edits = [
                {
                    'original_sentence': e.get('original_sentence', ''),
                    'inappropriate_part': e.get('inappropriate_part'),
                    'rewritten_part': e.get('rewritten_part')
                }
                for e in scored_edits
                if e.get("valid") == True
            ]
            argument_after_all_edits = apply_edits_to_argument(all_valid_edits, sentences, argument)

            # Argument-level classifier metrics
            scores_before = appropriateness_scorer.get_appropriateness_scores(argument) if appropriateness_scorer else {}
            scores_after = appropriateness_scorer.get_appropriateness_scores(argument_after_edits) if appropriateness_scorer else {}
            scores_after_all = appropriateness_scorer.get_appropriateness_scores(argument_after_all_edits) if appropriateness_scorer else {}

            # Track flips per dimension
            flipped = False
            for dim in _ANALYSIS_DIMS:
                before_val = scores_before.get(dim)
                after_val = scores_after.get(dim)
                if before_val is not None and after_val is not None and after_val < 0.5:
                    flips_per_dim[dim] += 1
                    if dim == "Inappropriateness":
                        flipped = True

            flipped_all = False
            for dim in _ANALYSIS_DIMS:
                before_val = scores_before.get(dim)
                after_val_all = scores_after_all.get(dim)
                if before_val is not None and after_val_all is not None and after_val_all < 0.5:
                    if dim == "Inappropriateness":
                        flipped_all = True

            # BERTScore similarity
            try:
                _, _, f1_sim = bert_scorer.score([argument_after_edits], [argument])
                sim_score = float(f1_sim.item())
            except Exception:
                sim_score = 0.0

            try:
                _, _, f1_sim_all = bert_scorer.score([argument_after_all_edits], [argument])
                sim_score_all = float(f1_sim_all.item())
            except Exception:
                sim_score_all = 0.0

            # NES
            nes_score = _normalized_edit_similarity_words(argument, argument_after_edits)
            nes_score_all = _normalized_edit_similarity_words(argument, argument_after_all_edits)

            # Perplexity
            try:
                ppl_score = calculate_text_perplexity(argument_after_edits)
            except Exception:
                ppl_score = float("inf")

            try:
                ppl_score_all = calculate_text_perplexity(argument_after_all_edits)
            except Exception:
                ppl_score_all = float("inf")

            # Geometric mean
            app_bin = 1.0 if flipped else 0.0
            inv_ppl = 0.0 if not np.isfinite(ppl_score) or ppl_score <= 0 else (1.0 / ppl_score)
            try:
                gm_score = float((app_bin * sim_score * inv_ppl) ** (1 / 3)) if app_bin > 0 and sim_score > 0 and inv_ppl > 0 else 0.0
            except Exception:
                gm_score = 0.0

            app_bin_all = 1.0 if flipped_all else 0.0
            inv_ppl_all = 0.0 if not np.isfinite(ppl_score_all) or ppl_score_all <= 0 else (1.0 / ppl_score_all)
            try:
                gm_score_all = float((app_bin_all * sim_score_all * inv_ppl_all) ** (1 / 3)) if app_bin_all > 0 and sim_score_all > 0 and inv_ppl_all > 0 else 0.0
            except Exception:
                gm_score_all = 0.0

            # Global scorer metrics (perfect edits)
            global_ss_binary, global_ss_score = 0.0, 0.0
            global_hl_binary, global_hl_perplexity = 0.0, float('inf')
            global_fluency_binary, global_fluency_confidence = 0.0, 0.0

            try:
                global_ss_binary, global_ss_score = global_semantic_similarity_scorer.calculate_global_semantic_similarity(
                    argument, argument_after_edits
                )
            except Exception as e:
                logger.error(f"Global semantic similarity (perfect) failed: {e}")

            try:
                global_hl_binary, global_hl_perplexity = global_human_like_scorer.calculate_global_human_likeness(
                    argument, perfect_edits
                )
            except Exception as e:
                logger.error(f"Global human-likeness (perfect) failed: {e}")

            try:
                global_fluency_binary, global_fluency_confidence = global_fluency_scorer.calculate_global_fluency(
                    argument, argument_after_edits
                )
            except Exception as e:
                logger.error(f"Global fluency (perfect) failed: {e}")

            # Global scorer metrics (all valid edits)
            global_ss_binary_all, global_ss_score_all = 0.0, 0.0
            global_hl_binary_all, global_hl_perplexity_all = 0.0, float('inf')
            global_fluency_binary_all, global_fluency_confidence_all = 0.0, 0.0

            try:
                global_ss_binary_all, global_ss_score_all = global_semantic_similarity_scorer.calculate_global_semantic_similarity(
                    argument, argument_after_all_edits
                )
            except Exception as e:
                logger.error(f"Global semantic similarity (all) failed: {e}")

            try:
                global_hl_binary_all, global_hl_perplexity_all = global_human_like_scorer.calculate_global_human_likeness(
                    argument, all_valid_edits
                )
            except Exception as e:
                logger.error(f"Global human-likeness (all) failed: {e}")

            try:
                global_fluency_binary_all, global_fluency_confidence_all = global_fluency_scorer.calculate_global_fluency(
                    argument, argument_after_all_edits
                )
            except Exception as e:
                logger.error(f"Global fluency (all) failed: {e}")

            # Predicted scores
            pred_scores_before: Dict[str, float] = {}
            pred_scores_after: Dict[str, float] = {}
            pred_scores_after_all: Dict[str, float] = {}

            for dim in _ANALYSIS_DIMS:
                val_before = scores_before.get(dim)
                if val_before is not None:
                    pred_scores_before[dim] = 1.0 if float(val_before) >= 0.5 else 0.0

                val_after = scores_after.get(dim)
                if val_after is not None:
                    pred_scores_after[dim] = 1.0 if float(val_after) >= 0.5 else 0.0

                val_after_all = scores_after_all.get(dim)
                if val_after_all is not None:
                    pred_scores_after_all[dim] = 1.0 if float(val_after_all) >= 0.5 else 0.0

            # Write output record
            record = {
                "post_id": post_id,
                "issue": issue,
                "argument": argument,
                "argument_after_edits": argument_after_edits,
                "argument_after_all_edits": argument_after_all_edits,
                "metrics": {
                    "App": app_bin,
                    "Sim": sim_score,
                    "NES": nes_score,
                    "PPL": ppl_score,
                    "GM": gm_score,
                },
                "metrics_all": {
                    "App": app_bin_all,
                    "Sim": sim_score_all,
                    "NES": nes_score_all,
                    "PPL": ppl_score_all,
                    "GM": gm_score_all,
                },
                "global_scores": {
                    "semantic_similarity_binary": global_ss_binary,
                    "semantic_similarity_score": global_ss_score,
                    "human_like_binary": global_hl_binary,
                    "human_like_perplexity": global_hl_perplexity,
                    "fluency_binary": global_fluency_binary,
                    "fluency_confidence": global_fluency_confidence,
                },
                "global_scores_all": {
                    "semantic_similarity_binary": global_ss_binary_all,
                    "semantic_similarity_score": global_ss_score_all,
                    "human_like_binary": global_hl_binary_all,
                    "human_like_perplexity": global_hl_perplexity_all,
                    "fluency_binary": global_fluency_binary_all,
                    "fluency_confidence": global_fluency_confidence_all,
                },
                "predicted_scores_before": pred_scores_before,
                "predicted_scores_after": pred_scores_after,
                "predicted_scores_after_all": pred_scores_after_all,
                "edits": scored_edits,
            }
            f_out.write(json.dumps(record, ensure_ascii=False) + "\n")

            # Progress logging
            if idx < 3 or (idx + 1) % 50 == 0 or (idx + 1) == num_examples:
                elapsed = time.time() - example_start
                logger.info(
                    f"[{idx + 1}/?] eval={elapsed:.1f}s edits={len(scored_edits)} valid={valid_count} perfect={perfect_ones}"
                )

    # Compute flip percentages
    flip_percentages = {dim: (flips_per_dim[dim] / max(1, num_examples)) for dim in _ANALYSIS_DIMS}

    logger.info(
        f"Finished. Time={time.time() - start_all:.1f}s | examples={num_examples}"
        f" | edits={total_edits} valid_edits={total_valid_edits} perfect={total_perfect_reward_ones}"
        f" | reason_correct={total_reason_correct} ({total_reason_correct / max(1, total_edits):.1%})"
        f" | App%={flip_percentages.get('Inappropriateness', 0.0):.2%}"
    )

    # Log per-dimension flip percentages
    logger.info("Flip percentages per dimension:")
    for dim in _ANALYSIS_DIMS:
        logger.info(f"- {dim}: {flip_percentages[dim]:.2%}")

    logger.info(f"Wrote {output_jsonl}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate edits with configurable scorers.")
    parser.add_argument("--input_jsonl", type=str, required=True, help="Path to input JSONL file with generated edits.")
    parser.add_argument("--output_jsonl", type=str, required=True, help="Path to output JSONL file with scored edits.")

    # Scorer toggles (thresholds are defined in scorer classes)
    parser.add_argument("--disable_semantic_similarity", action="store_true", help="Disable semantic similarity scorer.")
    parser.add_argument("--disable_fluency", action="store_true", help="Disable fluency scorer.")
    parser.add_argument("--disable_human_like", action="store_true", help="Disable human-like scorer.")
    parser.add_argument("--disable_appropriateness", action="store_true", help="Disable appropriateness scorer.")

    args = parser.parse_args()

    main(
        args.input_jsonl,
        args.output_jsonl,
        disable_semantic_similarity=args.disable_semantic_similarity,
        disable_fluency=args.disable_fluency,
        disable_human_like=args.disable_human_like,
        disable_appropriateness=args.disable_appropriateness,
    )
