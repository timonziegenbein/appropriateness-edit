# Gemini Code Assistant Context

## Project Overview

This project focuses on fine-tuning a large language model to identify and edit inappropriate text, ensuring the core message is preserved. The model is trained using Generative Reinforcement Policy Optimization (GRPO), a reinforcement learning technique.

The training process involves a sophisticated reward system that evaluates edits based on four key criteria:

*   **Semantic Similarity:** Ensures the edited text remains true to the original author's intent.
*   **Human-Likeness:** Assesses the naturalness and quality of the generated edits.
*   **Fluency:** Checks for grammatical correctness and readability.
*   **Appropriateness:** Measures the reduction of inappropriate content.

The model is prompted with a detailed set of instructions, including the text to be analyzed and the desired JSON output format. This structured approach allows for consistent and high-quality edits.

### Key Technologies

*   **Python:** The primary programming language.
*   **PyTorch:** The core machine learning framework.
*   **Hugging Face Transformers:** Used for loading and training the language model.
*   **Hugging Face TRL:** Provides the GRPO implementation.
*   **PEFT (Parameter-Efficient Fine-Tuning):** For efficient model training.
*   **Sentence Transformers:** For calculating semantic similarity.
*   **NLTK:** Used for sentence tokenization.

## Building and Running

### Training the Model

The main training script is `models/grpo.py`. To train the model, run the following command:

```bash
python models/grpo.py --model_name <model_name> --output_dir <output_dir>
```

*   `--model_name`: The name of the base model to train (e.g., `unsloth/Llama-3.1-8B-Instruct`).
*   `--output_dir`: The directory where the trained model will be saved.

### TODO: Add Testing Instructions

Instructions for running the project's tests are not yet available.

## Development Conventions

### Code Style

The code follows the PEP 8 style guide for Python.

### Logging

The project uses the `logging` module to log information about the training process. Logs are written to `training.log` and to the console.

### Reward Functions

The reward functions are defined in `scorers/reward_functions.py`. These functions are crucial for the GRPO training process and are designed to be modular and extensible.

### Prompts

The prompts for the language model are generated by the `create_llm_prompt` function in `prompts/edit_inappropriate_text.py`. This function creates a detailed prompt that includes the task description, the expected output format, and the definitions of inappropriateness.
